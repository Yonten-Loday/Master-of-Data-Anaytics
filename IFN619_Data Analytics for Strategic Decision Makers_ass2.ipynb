{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3259fdfd-0ec0-4835-b782-f2277525bc2a",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFEE; color:#440404; padding:8px; border-radius: 4px; text-align: center; font-weight: 500;\">IFN619 - Data Analytics for Strategic Decision Makers (2024 Sem 1)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823904d-fea6-46d0-9b3a-798d23addb01",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IFN619 :: UA2 - Extending Analytics (40%)\n",
    "\n",
    "**IMPORTANT:** Refer to the instructions in Canvas [UA2 - Assignment 2 - extending analytics](https://canvas.qut.edu.au/courses/17432/assignments/163774) *BEFORE* working on this assignment.\n",
    "\n",
    "#### REQUIREMENTS ####\n",
    "\n",
    "1. Complete and run the code cell below to display your name, student number, and assignment option\n",
    "2. Identify an appropriate question (or questions) to be addressed by your overall data analytics narrative\n",
    "3. Extend your analysis in assignment 1 with:\n",
    "    - the analysis of additional unstructured data using the Guardian API (See accessing the Guardian API notebook),\n",
    "    - the use of one machine learning technique (as used in the class materials), and\n",
    "    - identification of ethical considerations relevant to the analysis (by drawing on class materials).\n",
    "4. Ensure that you include documentation of your thinking and decision-making using markdown cells\n",
    "5. Ensure that you include appropriate visualisations, and that they support the overall narrative\n",
    "6. Ensure that your insights answer your question/s and are appropriate to your narrative. \n",
    "7. Ensure that your insights are consistent with the ethical considerations identified.\n",
    "\n",
    "**NOTE:** you should not repeat the analysis from assignment 1, but you may need to save dataframes from assignment 1 and reload for use in this assignment. You may also summarise your assignment 1 insights as part of the process of identifying questions for analysis.\n",
    "\n",
    "#### SUBMISSION ####\n",
    "\n",
    "1. Create an assignment 2 folder named in the form **UA2-surname-idnumber** and put your notebook and any data files inside this folder. Note, do not put large training data in this folder (reference any training data that you used but keep it outside this folder), only keep small data files and models in this folder with your notebook.\n",
    "2. When you have everything in the correct folder, reset all cells and restart the kernel, then run the notebook completely, checking that all cells have run without error. If you encounter errors, fix your notebook and re-run the process. It is important that your notebook runs without errors only requiring the files in the folder that you have created.\n",
    "3. When the notebook is error free, zip the entire folder (you can select download folder in Jupyter).\n",
    "4. Submit the zipped folder on Canvas [UA2 - Assignment 2 - extending analytics](https://canvas.qut.edu.au/courses/17432/assignments/163774)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ac7db-0344-4890-b93f-8cd5842a5e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following cell with your details and run to produce your personalised header for this assignment\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "# personal details\n",
    "first_name = \"Yonten\"\n",
    "last_name = \"Loday\"\n",
    "student_number = \"N11828773\"\n",
    "\n",
    "personal_header = f\"<h1>{first_name} {last_name} ({student_number})</h1>\"\n",
    "HTML(personal_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189f2f6-4596-4521-a991-fe39d1ee92ca",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Advance Queensland Program and Grants is an initiative by the Queensland Government with an aim to promote innovation, economic growth, and job creation within Queensland. The first findings (assessment 1) have clearly shown that there is a unequal fund distribution between South East Queensland and Regional Queensland. South-East Queensland received considerably more funds compared to Regional Queensland, raising concerns about the fairness and effectiveness of the current fund distribution policy. And the number of funding recipients have reduced drastically since the beginning of 2021. Through those findings, we have understood the patterns, trends and biases in fund allocation. Based on the insights from assessment 1, this report will look in to the following questions. \n",
    "1. What is the reaction from public on the funding distribution? (Sentiment Analysis)\n",
    "2. Can Machine Learning confirm the regional funding disparity? (K-Means Clustering)\n",
    "3. Why is there a significant difference in fund allocation between South-East Queensland and Regional Queensland? (NMF)\n",
    "\n",
    "To address these questions, I will follow QDAVI cycle of data analytics, incorporating  unstructured data from the Guardian API, applying a machine learning techniques to uncover deeper insights, and considering the ethical implications of the findings. Through this comprehensive approach, I aim to provide a robust narrative that not only highlights the existing issues but also offers policy recommendations for improving fund distribution policies. \n",
    "\n",
    "Ethical considerations are integral to ensure that the results and outcomes are just, transparent, and beneficial for all stakeholders. Adhering to ethical standards, analysts can produce reliable, fair, and actionable insights that contribute positively to decision-making processes and societal well-being. Therfore, this report has incorporated crucial ethical aspects and are identified when used (just before the implementation code cell) with note **Ethical Consideration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e60233-29a3-48b9-885d-b64e4b2ead75",
   "metadata": {},
   "source": [
    "--- -----------------------------------------------------------------------------------------------------------------------\n",
    "I will start by importing of all required libraries, the use of the libraries are marked as comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7413329-4903-45a8-a4d5-b2880a5b2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json data and dataframe\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#text analysis\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "#visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "#machine learning \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea729db7-9cee-4e62-9424-7d47bf50e365",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "**What is the reaction from public on the funding distribution? (Sentiment Analysis)**\n",
    "\n",
    "Importance of the question: It is important to understand the opinions of th public before starting the indepth analysis. It will give overview of the analysis. For instance, if the sentiment of the public is negative or close to negative, it will prepare us what to look into next. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c35397-d62a-4e35-962f-3b1b7d829582",
   "metadata": {},
   "source": [
    "## Data\n",
    "In order to answer the first question, I will use articles (unstructured data) from Guardian API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15fcbdc-1f9d-4893-9277-dd9cdb6e1206",
   "metadata": {},
   "source": [
    "**Ethical Considerations**: API Key Management & Data Privacy</br>\n",
    "Before requesting the articles from Guardian API, I have to set up my api key which is in private folder with key.txt file. This ensures that access credentials are not exposed publicly, protecting the data source from unauthorized access. The articles retrieved from the Guardian API are publicly available information, ensuring compliance with data privacy standards. No personal data is used or exposed in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb8906-dd1b-408e-adbc-80d764f1b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../private/key.txt', 'r') as file:\n",
    "    key = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283cec4-fa35-47b4-a8aa-d2f0f759e09f",
   "metadata": {},
   "source": [
    "To get the public reaction, I want to do sentiment analysis on the content (bodyText) of the articles. Sentiment analysis will give me how positve, negative, or neutral the public opinion has on the program. \n",
    "\n",
    "I also want to check if different number of articles result different sentiments. So, the following code cell has two dictionaries to request two different-sized articles, one with 100 articles (params_100) and another with 200 articles (params_200). The only difference is page-size. As for the sentiment analysis, I want only headline (title), trailText(summary/highlights), bodyText, and webUrl(for question 3) of the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e7acf-3e93-417d-8e4d-5af79412eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://content.guardianapis.com/search'\n",
    "\n",
    "params_100 = {\n",
    "    'q': 'Queensland AND (economic growth OR fund allocation OR regional development OR south east)',\n",
    "    'api-key': key,\n",
    "    'page-size': 100,  \n",
    "    'show-fields': 'headline,trailText,bodyText,webUrl'   \n",
    "}\n",
    "\n",
    "params_200 = {\n",
    "    'q': 'Queensland AND (economic growth OR fund allocation OR regional development)',\n",
    "    'api-key': key,\n",
    "    'page-size': 200,  \n",
    "    'show-fields': 'headline,trailText,bodyText' \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5ff12-63c6-47e0-a87f-0cd07293a3b4",
   "metadata": {},
   "source": [
    "I then request Guardian API for articles related to my 'q': 'Queensland AND (economic growth OR fund allocation OR regional development OR south east)' with number of articles specified. Then stored the json data in data_100 and data_200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c2fdd-8d8f-4903-ae20-dea24ffd9bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make the API request\n",
    "response_100 = requests.get(base_url, params=params_100)\n",
    "data_100 = response_100.json()\n",
    "\n",
    "response_200 = requests.get(base_url, params=params_200)\n",
    "data_200 = response_200.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa446c3e-fd19-47e0-bc97-a617b6e9c450",
   "metadata": {},
   "source": [
    "The following code cell prints the unique keys inside ['response']['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d77ebe-6e37-411a-8d14-8f66027a720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_100 = data_100['response']['results']\n",
    "# Collect unique keys from each dictionary in the 'results' array\n",
    "unique_keys = set()\n",
    "fields_keys = set()\n",
    "for result in results_100:\n",
    "    unique_keys.update(result.keys())\n",
    "    fields_keys.update(result['fields'].keys())\n",
    "print(unique_keys)\n",
    "print(fields_keys) \n",
    "\n",
    "results_200 = data_200['response']['results']\n",
    "unique_keys = set()\n",
    "fields_keys = set()\n",
    "for result in results_200:\n",
    "    unique_keys.update(result.keys())\n",
    "    fields_keys.update(result['fields'].keys())\n",
    "print(\"------For 200 pages of artciles------\")\n",
    "print(unique_keys)\n",
    "print(fields_keys) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa0554-8965-4fcf-aca9-0d50f64a1d75",
   "metadata": {},
   "source": [
    "Then I collectd the relevant information of two different-sized articles using `append()` function. The collected information of the articles are then converted to data frame df_articles_100 and df_articles_200 using `pd.DataFrame()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab144084-636f-436c-9d16-6b542547ca59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract relevant information from the response\n",
    "articles_100 = []\n",
    "for result in data_100['response']['results']:\n",
    "    article = {\n",
    "        'headline': result['fields']['headline'],\n",
    "        'trailText': result['fields']['trailText'],\n",
    "        'bodyText': result['fields']['bodyText'],\n",
    "        'webUrl': result['webUrl']\n",
    "    }\n",
    "    articles_100.append(article)\n",
    "    \n",
    "articles_200 = []\n",
    "for result in data_200['response']['results']:\n",
    "    article = {\n",
    "        'headline': result['fields']['headline'],\n",
    "        'trailText': result['fields']['trailText'],\n",
    "        'bodyText': result['fields']['bodyText'],\n",
    "        'webUrl': result['webUrl']\n",
    "    }\n",
    "    articles_200.append(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728ffd1-d81f-420f-b025-ec67212f45b4",
   "metadata": {},
   "source": [
    "Then I printed the last 5 rows of each dataframe to see if I have collected different articles and the right number of articles. This is because last 5 articles of df_articles_100 should be different from df_articles_200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5d934-ca1a-4a0e-9de5-940e3a46024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_100 = pd.DataFrame(articles_100)\n",
    "df_articles_100.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876c5a1-7f7a-4b66-92e5-ef1128fcc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_200 = pd.DataFrame(articles_200)\n",
    "df_articles_200.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68cdc0f-5979-4555-92c9-89fabd2a77fb",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909e42e-a596-4fd4-95d4-c46d95d86157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81ad71-585e-4a98-ab76-7fd6e83c6016",
   "metadata": {},
   "source": [
    "I have downloaded the stopwords and punkt to proprocess the text data of the dataframes. English stop words and non-alphanumeric characters are removed becuase these characters and stop words do not add any value to the sentiment analysis. The text of bodyText is converted all to lower and then tokenized. These are all done by `preprocess_text()` function created. \n",
    "\n",
    "The tokens of both the dataframes are then joined back to form string for sentiment analysis compatability. The preprocessed bodyText is stored in new column called cleaned_text which I will be used hereafter (for sentiment analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa8d8d-69de-4507-a470-992fc49f4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply preprocessing to the bodyText column\n",
    "df_articles_100['cleaned_text'] = df_articles_100['bodyText'].apply(preprocess_text)\n",
    "df_articles_200['cleaned_text'] = df_articles_200['bodyText'].apply(preprocess_text)\n",
    "df_articles_200.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516db96-510a-46bf-9b13-b53cd2323cd9",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85192d3-3f4f-4135-b066-54a0a1b2bcbf",
   "metadata": {},
   "source": [
    "Before I do sentiment analysis, I want to provide readers a quick initial insights and visual summary of the prominent keywords and topics of the articles which will guide the readers to sentiment analysis. \n",
    "\n",
    "To do this, I created a word cloud using WordCloud library. To generate a word cloud, I first need frequency of each word. Therefore, the following cell code combines all text of cleaned_text column as one single string. Then, `Counter()` is used to get the frequency counter with each word and their number of occurence(frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed639558-de5c-47dd-89d9-7e69c6bb7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all articles into a single string\n",
    "all_text_100 = ' '.join(df_articles_100['cleaned_text'])\n",
    "all_text_200 = ' '.join(df_articles_200['cleaned_text'])\n",
    "\n",
    "# Get word frequency\n",
    "word_freq_100 = Counter(all_text_100.split())\n",
    "word_freq_200 = Counter(all_text_200.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3a485-2f26-450e-b3ba-abc839ee989c",
   "metadata": {},
   "source": [
    "The frequency counters is then fed to `generate_from_frequencies()` of WordCloud with other parameters for the word cloud [<a href = \"https://www.geeksforgeeks.org/generating-word-cloud-python/\">geeksforgeeks</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585354e-9aee-42c7-97f0-1d3ac877c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_100 = WordCloud(\n",
    "    width = 800,\n",
    "    height = 400,\n",
    "    max_font_size = 80, \n",
    "    background_color = 'white',\n",
    "    colormap = 'viridis'\n",
    ").generate_from_frequencies(word_freq_100)\n",
    "\n",
    "wordcloud_200 = WordCloud(\n",
    "    width = 800,\n",
    "    height = 400,\n",
    "    max_font_size = 80, \n",
    "    background_color = 'white',\n",
    "    colormap = 'viridis'\n",
    ").generate_from_frequencies(word_freq_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c8717-efac-4221-89f0-9e852f2670fc",
   "metadata": {},
   "source": [
    "The two wordclouds, first from 100 articles and another from 200 artcles are plotted together using matplotlib subplots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b0ca5-4eea-4d9d-b3c9-a0bc45c6eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# Plot the 100 articles word cloud\n",
    "axs[0].imshow(wordcloud_100, interpolation='bilinear')\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title('Word Cloud from 100 Articles')\n",
    "\n",
    "# Plot the 200 articles word cloud\n",
    "axs[1].imshow(wordcloud_200, interpolation='bilinear')\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title('Word Cloud from 200 Articles')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bec52d-91d2-45d6-8f6e-e5e15879164d",
   "metadata": {},
   "source": [
    "Insights from the Word Clouds of 100 and 200 Guardian Articles\n",
    "Common Themes:\n",
    "- In both word clouds, \"government,\" \"australia,\" \"said,\" \"minister,\" and \"people\" are among the most prominent words. This indicates that discussions around government, national affairs, and public statements are central themes in the articles.\n",
    "- Words like \"albanese\" (likely referring to Anthony Albanese, the Australian Prime Minister) and \"minister\" suggest a significant focus on political leadership and governmental figures.\n",
    "\n",
    "Word Cloud from 100 Articles:\n",
    "- Key Topics: In addition to the common themes, this word cloud highlights \"health,\" \"education,\" \"public,\" and \"funding.\" These topics suggest that a substantial portion of the articles focus on public services and funding issues.\n",
    "- Geographical Focus: The presence of words like \"queensland\" and \"south\" indicates some regional focus, particularly on Queensland and southern parts of Australia.\n",
    "- Specific Issues: Words such as \"energy,\" \"nuclear,\" \"school,\" and \"police\" suggest discussions on energy policies, law enforcement, and educational institutions.\n",
    "  \n",
    "Word Cloud from 200 Articles:\n",
    "- Expanded Topics: With a larger dataset, additional prominent words like \"NSW\" (New South Wales), \"court,\" \"report,\" and \"time\" emerge, indicating a broader range of topics covered, including legal matters and various time-bound events.\n",
    "- Public and Social Services: Similar to the 100 articles, there is a strong emphasis on \"public,\" \"health,\" and \"services,\" indicating consistent coverage of public welfare topics.\n",
    "- Government and Policy: The continued prominence of words like \"government,\" \"state,\" and \"federal\" suggests ongoing discussions about government policies and state-level governance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b361002-2369-424e-aac2-a3448ee6434c",
   "metadata": {},
   "source": [
    "Now that readers have brief insight to the overall articles with the help of wordcloud, I will do sentment analysis using `TextBlob()`. TextBlob is a popular library in Python for basic natural language processing tasks. It provides a simple and easy-to-use interface for performing basic sentiment analysis, making it suitable for quick and general-purpose tasks without needing extensive setup or customization \n",
    "[<a href ='https://textblob.readthedocs.io/en/dev/'>TextBlob Documentation</a>]\n",
    "\n",
    "Using textblob, we get the polarity and that is addded to the dataframe of each article length as new column called sentiment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ef6ea-cf4d-43f2-98b9-dd37e5dc0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentiment polarity\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df_articles_100['sentiment'] = df_articles_100['cleaned_text'].apply(get_sentiment)\n",
    "df_articles_100.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc735d2-ebd7-45cd-80da-9380842faf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis\n",
    "df_articles_200['sentiment'] = df_articles_200['cleaned_text'].apply(get_sentiment)\n",
    "df_articles_200.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9caedb-8f3e-4fb9-b2a6-c63f2c6f5079",
   "metadata": {
    "tags": []
   },
   "source": [
    "With the sentiment column, the polarity score of each articles, I want to make a visual representation of the sentiments distribution to my readers about the sentiments of the public. I am using seaborn subplots to plot two histograms with  Kernel Density Estimate (KDE), each histogram for different-sized articles. \n",
    "\n",
    "KDE plots smooth out data noise to reveal underlying patterns and provide a clearer view of data distribution, making it easier to compare peaks, spread, and overall shape between datasets. They complement histograms by offering a continuous estimate of probability density, enhancing the insights gained from data visualization. [<a href = \"https://seaborn.pydata.org/generated/seaborn.kdeplot.html\">seaborn.kdeplot</a>]\n",
    "\n",
    "**Ethical Considerations**</br>\n",
    "The number of bins are not randomly selected, it is calculated using Strurges' Rule. Sturges' Rule is widely employed data analysis technique to determine the number of bins for a histogram. Strurges' formula: </br> <img src = \"formula.png\" width = \"200\"/> where: \\( n \\) is the number of observations and ⌈⋅⌉ denotes eiling function (round up to the nearest integer)[<a href = \"https://en.wikipedia.org/wiki/Sturges%27s_rule#:~:text=This%20rule%20is%20widely%20employed,the%20default%20bin%20selection%20method.&text=(due%20to%20counting%20the%200,the%20result%20is%20rounded%20up.\">Wikipedia</a>]\n",
    "</br> \n",
    "*for 100 articles*</br>\n",
    "1+ log2(100) ≈ 7.644 = 8\n",
    "</br>*for 200 articles*</br>\n",
    "1+ log2(200) ≈ 8.644 = 9\n",
    "\n",
    "**Ethical Considerations** </br>\n",
    "As I have different number of articles, the visualization of 2 histograms will have no consistent x and y axis scale which will mislead the visualisation and it will be diffult to compare. So, I synchronise the y-axis and x-axis to derive accurate and meaningful visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc7346-f2bf-40e6-83b7-3f9bba80e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot the sentiment distribution for 100 articles\n",
    "sns.histplot(df_articles_100['sentiment'], bins=8, kde=True, ax=axs[0])\n",
    "axs[0].set_title('Sentiment Distribution of 100 Guardian Articles')\n",
    "axs[0].set_xlabel('Sentiment Polarity')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot the sentiment distribution for 200 articles\n",
    "sns.histplot(df_articles_200['sentiment'], bins=9, kde=True, ax=axs[1])\n",
    "axs[1].set_title('Sentiment Distribution of 200 Guardian Articles')\n",
    "axs[1].set_xlabel('Sentiment Polarity')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "\n",
    "# Synchronize y-axis limits\n",
    "max_freq = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1])\n",
    "axs[0].set_ylim(0, max_freq)\n",
    "axs[1].set_ylim(0, max_freq)\n",
    "\n",
    "# Synchronize x-axis limits\n",
    "min_x = min(axs[0].get_xlim()[0], axs[1].get_xlim()[0])\n",
    "max_x = max(axs[0].get_xlim()[1], axs[1].get_xlim()[1])\n",
    "axs[0].set_xlim(min_x, max_x)\n",
    "axs[1].set_xlim(min_x, max_x)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1ed64-792a-43d0-b402-fbf3ede34d3f",
   "metadata": {},
   "source": [
    "## Insights\n",
    "- Both distributions are centered around a sentiment polarity of approximately 0.1, indicating that the average sentiment of Guardian articles tends to be neutral to mildly positive. This central tendency is consistent across both the smaller sample of 100 articles and the larger sample of 200 articles. However, the sentiment polarity also spreads around -0.1 which is a concern for the program although the frequency is low. \n",
    "- The highest frequency of articles is found around a sentiment polarity of 0.1 in both graphs. This peak of KDE curve indicates that mildly positive sentiment is the most common sentiment observed in Guardian articles. The increased sample size in the 200-article graph results in a more pronounced and smoother peak, confirming the trend seen in the smaller sample.\n",
    "- The slightly broader range in the 200-article graph suggests a more diverse sample but maintains a similar sentiment spread. This indicates that the increase in number of articles do not affect the sentiment of public, it just smooths the spread. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e5186-5214-4748-9e28-277ce2e17492",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "**Can Machine Learning confirm the regional funding disparity?**\n",
    "\n",
    "Importance of Question 2: Accurate identification of funding disparities ensures that resources can be reallocated more equitably, promoting balanced regional development and reducing economic inequalities. Although the disparity is seen in assesment 1, this approach using machine learning further confirms the regional funding disparity.The confirmation of the disparity will answer the **sentiment/reaction** of the public.  \n",
    "\n",
    "To answer this, I will be using K-Means clustering. K-Means clustering is suitable for analyzing regional funding disparity due to its ability to handle multi-dimensional data, discover patterns, and provide clear and interpretable results. Its efficiency and scalability allows analysts to confirm and visualize funding disparities objectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ec590-c162-4e17-bda9-27006bcfae25",
   "metadata": {},
   "source": [
    "## Data\n",
    "For K-Means Clustering, I will be using cleaned data of Advance Queensland Funding Recipients from assessment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb0954-23ba-45d7-a4d2-5f83f00e3c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_data = pd.read_csv('fund_data.csv')\n",
    "print(fund_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15652ffa-2871-42e8-8166-b80a138f4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c2b49-a88c-45f4-841b-2ab55d809212",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0091f8-3123-4a89-89af-d5ed1ed772c4",
   "metadata": {},
   "source": [
    "The cleaned data has four columns, but only Region_Category\tand Actual Contractual Commitment($) are valuable for my K-Means Clustering. Therefore, I choose only those two with the following code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0849475-6510-4984-aafa-4a3b3b5168bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cols = ['Region_Category','Actual Contractual Commitment($)']\n",
    "fund_data = fund_data[imp_cols]\n",
    "fund_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8d0b9-87fc-48b1-b7ea-324b21500df1",
   "metadata": {},
   "source": [
    "For simplicity, I changed Region_Category to Region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b0289-199a-4286-984b-35e80a846e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_data = fund_data.rename(columns = {'Region_Category':'Region'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197dc38-5e80-40d9-8d10-460e0a0bf26d",
   "metadata": {},
   "source": [
    "The Region as we can see is categorical, so I convert that categorical labels using `LabelEncoder()`. LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1 [<a href = \"https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets\">Label Encoding </a>]\n",
    "\n",
    "Regional Queensland: Region 0 </br>\n",
    "South East Queesnsland: Region 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d69a5-d1f0-4863-9c7c-4d8d7459b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categoridfcal data\n",
    "label_encoder = LabelEncoder()\n",
    "fund_data['Region'] = label_encoder.fit_transform(fund_data['Region'])\n",
    "fund_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec6a1e-acd0-46f4-b906-29b81401ca73",
   "metadata": {},
   "source": [
    "**Ehtical Considerations** : Fair Clustering</br>\n",
    "Then I standardized the Actual Contractual Commitment($) column using `StandardScaler()` of sklearn. This step is important because standardizing before applying K-Means clustering ensures that each feature contributes equally to the distance calculations used to form clusters. This process converts the features to a common scale, preventing features with larger ranges from disproportionately influencing the clustering results. Standardization therefore, improves the accuracy and efficiency of the clustering algorithm, enabling it to correctly identify underlying patterns in the data without bias towards any particular feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423eebbc-ee79-4bab-9eec-231549d493d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the numerical data\n",
    "scaler = StandardScaler()\n",
    "fund_data['Actual Contractual Commitment($)'] = scaler.fit_transform(fund_data[['Actual Contractual Commitment($)']])\n",
    "\n",
    "fund_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58752a-e77a-4d84-b7e8-2e1e15f1b09a",
   "metadata": {},
   "source": [
    "**Ethical Considerations**</br>\n",
    "One of the important parameters of KMeans clustering is n_clusters which is a number of clusters to which we want the clustering to be done. There are multiple ways of selecting the appropriate n_clusters from randomly selecting to complex algorithms. For my K-Means Clustering, I used Elbow method. It involves plotting the variance explained by different numbers of clusters and identifying the “elbow” point, where the rate of variance decreases sharply levels off, suggesting an appropriate cluster count for analysis or model training. To plot the elbow graph, we need inertia which is a sum of squared distances between each data point and the nearest cluster centroid. \n",
    "\n",
    "The following code cell generates interia to plot elbow graph. And using matplotlib and inertia, I plotted the elbow graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e9571-bbbe-4a2a-9877-146d001a4fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inertia = []\n",
    "scaled_data = fund_data.values\n",
    "\n",
    "for n in range(1, 11): #(number of clusters trial)\n",
    "    kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertia.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a62976-242b-45c4-9177-5ab30f674dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7f209-5639-4f2e-acc2-230376d16d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Determining Optimal Number of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b08642-ca34-4759-819c-987b490f3caf",
   "metadata": {},
   "source": [
    "Based on the elbow plot, the optimal (appropriate) number of clusters for this data is 3 because the variance decreases sharply levels off at 3. And Kmeans is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aaf77f-fd5f-4cd7-84f9-e7b01e0e67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_clusters = 3\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "fund_data['cluster'] = kmeans.fit_predict(scaled_data)\n",
    "fund_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d073dd3f-69de-46ed-a34a-1b2564918960",
   "metadata": {},
   "source": [
    "Then plotted the kmeans clustering of fund allocations between seq and rq to visualise the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e10b7-a882-4124-ad7e-90e22cd6694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(fund_data, x='Region', y='Actual Contractual Commitment($)', color='cluster',\n",
    "                 title='K-Means Clustering of Fund Allocation between SEQ and RQ',\n",
    "                 labels={'Region': 'Region', 'Actual Contractual Commitment($)': 'Fund Amount (A$)'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d828417-cb1e-4806-815a-626bdbe1c373",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "<b>Cluster 0 (Blue):</b>\n",
    "This cluster primarily consists of funding amounts allocated to South-East Queensland (SEQ) with a a lower range of funding amounts. \n",
    "</br><b>Cluster 1 (Pink):</b>\n",
    "This cluster also consists of funding amounts allocated to SEQ with more dispersed and higher funding amounts compared to Cluster 0.\n",
    "</br><b>Cluster 2 (Yellow):</b>\n",
    "This cluster is associated with Regional Queensland (RQ).The data points are tightly grouped together, suggesting a consistent and lower range of funding allocations for RQ. \n",
    "\n",
    "The insights from assignment 1 were confirmed here with the machine learning (K-Means clustering). The K-Means clustering analysis provides clear evidence of the regional funding disparities between South-East Queensland and Regional Queensland. SEQ receives a broader range of funding amounts, with some projects receiving significantly higher allocations. In contrast, RQ receives lower funding amounts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e5ef9-14e6-490e-a4f4-d7e008dc49bf",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "**Why is there a significant difference in fund allocation between South-East Queensland and Regional Queensland? (NMF)**\n",
    "\n",
    "Importance of the question: After understanding that there is a significant funding allocation disparing between SEQ and RQ, it is important to understand what caused such disparity. \n",
    "\n",
    "To answer this question, I will use NMF (Non-negative Matrix Factorization) with Guardian articles for topic modelling. With this I will identify key topics within the articles. In the end, I will select top articles, and then filter only those articles related to politics, Queensland, Regional Queensland, and economy. I will review the article manually to see the appropriate reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a553bab7-aac2-4c2b-8cb6-3c2a6be3128c",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "I will use the cleaned 200 articles data from **Question 1**. I am choosing 200 because it increase the chances of getting more articles related to the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd9fee6-72e9-4f15-adad-73852d9a6f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = df_articles_200\n",
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5617c82a-f1b4-48b5-a8fd-62e55a860061",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "All the cleaning inlcuding the removal of stop words, non-alphanumeric characters, lowercasing, and tokenizing are done in the analysis part of question1. \n",
    "\n",
    "The following code cell verctorises the text with 95% occurrence in the documents and occurrence in more than 2 documents. The vectorization is done on the cleaned_text column of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2958069-a5aa-455d-97bc-67f688dd72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df_articles['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c27d33e-cd79-4b17-bd72-cd71cc2bdc2b",
   "metadata": {},
   "source": [
    "Then NMF is applied to generate 5 distinct topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c005f-20a5-4c73-b390-248d9ccfe93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72699b2d-96cf-45f9-a194-f85284df963e",
   "metadata": {},
   "source": [
    "The following code cell displays the top words for each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad463d9-852e-47a9-9513-5605fd5bddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nmf_topics(model, feature_names, no_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topics[topic_idx] = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return topics\n",
    "\n",
    "no_top_words = 10\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "nmf_topics = display_nmf_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "\n",
    "# Display the topics\n",
    "for topic, words in nmf_topics.items():\n",
    "    print(f\"Topic {topic}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d72fda-6893-4a6c-be9e-3f5e378a31b7",
   "metadata": {},
   "source": [
    "Now based on the topics, I want to generate top Guardian Articles with their respective webUrls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3e47c-124e-4645-9ba6-7b6c2a1d8e37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform the document-term matrix using the NMF model to get the topic distribution\n",
    "topic_distribution = nmf.transform(tfidf)\n",
    "\n",
    "# Function to get the top N articles for each topic\n",
    "def get_top_articles_per_topic(topic_distribution, df_articles, top_n=10):\n",
    "    top_articles = {}\n",
    "    for topic_idx in range(topic_distribution.shape[1]):\n",
    "        top_article_indices = np.argsort(topic_distribution[:, topic_idx])[::-1][:top_n]\n",
    "        top_articles[topic_idx] = df_articles.iloc[top_article_indices][['webUrl']]\n",
    "    return top_articles\n",
    "\n",
    "# Get the top 10 articles for each topic\n",
    "top_articles_per_topic = get_top_articles_per_topic(topic_distribution, df_articles, top_n=10)\n",
    "\n",
    "# Display the top article URLs for each topic\n",
    "for topic, articles in top_articles_per_topic.items():\n",
    "    print(f\"\\nTopic {topic}:\")\n",
    "    for idx, row in articles.iterrows():\n",
    "        print(f\"- URL: {row['webUrl']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f033336-9f14-4caf-bf28-57f045d08ea5",
   "metadata": {},
   "source": [
    "As expected, not alltop articles are related to regional funding disparity. Therefore, I want to filter and retrieve only the articles that are related to politics, Queensland, regional Queensland and Economy. The following code cell does this filtering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533e4d2-c372-444b-a680-8565e4fed120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords for filtering\n",
    "keywords = ['politics', 'Queensland', 'Regional Queensland', 'economic']\n",
    "\n",
    "def filter_articles(df_articles, keywords):\n",
    "    filtered_articles = []\n",
    "    for idx, row in df_articles.iterrows():\n",
    "        if any(keyword.lower() in row['webUrl'].lower() for keyword in keywords):\n",
    "            filtered_articles.append(row['webUrl'])\n",
    "    return filtered_articles\n",
    "\n",
    "# Filter the top articles\n",
    "for topic, articles in top_articles_per_topic.items():\n",
    "    print(f\"\\nTopic {topic}:\")\n",
    "    filtered_urls = filter_articles(articles, keywords)\n",
    "    for url in filtered_urls:\n",
    "        print(f\"- URL: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2671b0f-a28f-4eb8-8ed5-1bc7567f0b5b",
   "metadata": {},
   "source": [
    "## Insights\n",
    "I have reviewed the articles and following are the findings:\n",
    "1. *\"Queensland government approves <b> Winchester South mine </b> despite report warning of potential ‘climate change consequences’\"*- This is an    <a href='https://www.theguardian.com/australia-news/2024/feb/07/winchester-south-mine-queensland-federal-government-approval-emissions-whitehaven'>excerpt</a> from an article. With mining works approved in one of the Regional Queensland towns, it will create many <b> job opportunities, improve livelihood of the region, and impact the community</b>. Although the full article reveals that the environmentalist have warned the government not to approve due to climate changing issues, but introducing such program in the regional areas will help the people of that community if we take into <b> economical benefits </b> it brings. This must have improved the sentiment of the public.\n",
    "   \n",
    "2. *\"South East Queensland’s population is expected to grow by 2.2 million people by 2046. The state government, in consultation with communities, industry and other key stakeholders, has created a plan to respond to this growth.\"* - This is an <a href = \"https://www.theguardian.com/qld-gov-shapingseq/2024/apr/08/the-future-of-south-east-queensland-how-a-new-plan-is-supporting-a-fast-growing-region\"> excerpt</a>. This article shows one of the many **reasons why funding are mostly allocated in SEQ**. With growing population, there is a need to have a proper plan (money) to avoid obstacles. Queensland government must have kept and spent more on SEQ for this reason.\n",
    "   \n",
    "3. *\"Australia’s economic growth slows in March quarter despite rise in household spending\"*- This is an <a href = \"https://www.theguardian.com/business/2022/jun/01/australias-economic-growth-slows-in-march-quarter-despite-rise-in-household-spending\">excerpt</a> from an article. This article has helped me to answer my insight from assignment 1 about the decreasing number of funding recipients. Due to the **slow economic growth**, the government must have cut some of the budgets on this program. \"In the first three months of 2022, gross domestic product rose to an annual rate of 3.3%, the Australian Bureau of Statistics said. That eased from the earlier reported annual pace of 4.2% and compared with about 3% forecast by economists.\"- from the same article. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e948514-b619-4225-98e7-150ebda9eb0e",
   "metadata": {},
   "source": [
    "# Policy Recommendations:\n",
    "- Equitable Distribution: Implement policies that ensure equitable distribution of funds based on clearly defined and transparent criteria that consider the unique needs of different regions within RQ. The equal distribution of funds to SEQ and RQ will be impossible considering the population, necessity, importance and economy. The SEQ will obiviously have more fund allocation, but the distribution should be equitable that RQ also receives what those regions have to receive.\n",
    "  \n",
    "- Periodic Reviews: Conduct periodical reviews of fund allocation policies to assess their effectiveness and make necessary adjustments based on feedback and changing regional needs.\n",
    "  \n",
    "- Stakeholder Involvement: Involve local communities and stakeholders in the decision-making process to ensure that the fund distribution policy is inclusive and addresses the actual needs of the regions.\n",
    "  \n",
    "- Transparency and Accountability: Enhance transparency in the fund allocation process by publicly sharing criteria, decision-making processes, and outcomes. This can help build public trust and ensure accountability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed41b6e-d772-4fad-846a-9cf448bb2f57",
   "metadata": {},
   "source": [
    "# Other Ethical Considerations \n",
    "- Methodology Documentation: The project includes clear documentation of all steps, including data collection, preprocessing, analysis, visualisations, and insights. This transparency allows for reproducibility and exploration by others, ensuring that the analysis can be independently verified.\n",
    "\n",
    "- Clear visualisation: The use of clear and interpretable visualizations, such as word clouds, sentiment distribution histograms, and K-Means Clustering helps communicate findings effectively and transparently to the audience.\n",
    "\n",
    "- Ethical Data Handling: The data from the Guardian API is used responsibly, with a clear understanding that it is intended for analysis and insights rather than any misuse.\n",
    "\n",
    "- Avoiding Misinterpretation: The project ensures that the findings are presented accurately to avoid misinterpretation that could lead to harmful decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6a6ab-c9fe-49e5-8bf5-6ffdb318d367",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735849a9-0b07-4203-ab81-195f51ffc79b",
   "metadata": {},
   "source": [
    "The comprehensive analysis of the Advance Queensland Program and Grants reveals significant regional disparities in funding allocation between South-East Queensland (SEQ) and Regional Queensland (RQ). Common themes in Guardian articles highlight the centrality of government actions and public statements, with a strong focus on political leadership and public services. Sentiment analysis shows a neutral to mildly positive public reaction, with a consistent sentiment spread regardless of sample size.\n",
    "\n",
    "K-Means clustering analysis confirms the initial findings, demonstrating that SEQ receives a broader and higher range of funding amounts, while RQ receives lower funding amounts. This disparity is supported by various articles: the approval of the Winchester South mine in RQ is seen as a job creator, while SEQ's growing population justifies higher funding to manage growth effectively. Additionally, the overall economic slowdown has led to budget cuts, affecting the number of funding recipients in the recent years.\n",
    "\n",
    "The narrative underscores the importance of equitable fund distribution to promote balanced regional development and reduce economic inequalities across Queensland. By addressing these disparities and considering the broader socio-economic context, policymakers can make informed decisions (policy recommendations) to support both SEQ and RQ, ensuring sustainable growth and development for all regions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
